---
title: "Bering Sea and Aleutian Islands biomass and catches"
format: html
embed-resources: true
editor: visual
---

## Goal

To collect, compile, and analyze fish community catch data, landings, and biomass estimates from a variety of sources. This includes integrating datasets from fisheries catch statistics and scientific surveys to quantify catch and biomass across different regions. The goal is to develop a dataset, including both Large Marine Ecosystems (LMEs) and Marine Ecoregions (MEs), that facilitates the validation and calibration of marine ecosystem models with a focus on fish and fisheries.

## Data types

-   Catches and landings by LME from Sea Around us Data (version 50.1)

-   Catches and landings from Watson (2017, [10.1038/sdata.2017.39](10.1038/sdata.2017.39)) aggregated to LME-scale

-   NOAA stock assessment database (XXXX)."

-   Fisheries independent trawl survey data of fish biomass - DOI [10.5281/zenodo.7992513](https://doi.org/10.5281/zenodo.7992513)

```{r}
#| echo: true
#| warning: false
#| message: false
#| error: false

# define values
LME_name   <- "East Bering Sea"
Start_year <- 1984
End_year <- 2015

# load libraries
library(ggplot2)
library(dplyr)
library(sf)
library(tidyverse)
library(tidyr)
library(sdmTMB)
```

## Catches and landings

Time series of marine fish catches and landings were estimated by LME using data from SAU and Watson (2017). The catch estimates encompass landings, discards, and approximations of illegal, unregulated, and unreported (IUU) catches.

The landings figure includes a time series of landings for all assessed stocks for which total biomass or spawning stock biomass estimates are available in the Gulf of Alaska. Since the ecoregion boundaries overlap with the stock assessment boundaries, there was no need for a spatial allocation of biomass (as done for the ICES stocks in the EU regions).

```{r Catch & Land. }
# -----------------------------------------------------
# load SAU data
SAU <- read.csv("../Data/SAU/SAU_EastBeringSea_LME.csv")
SAU <- subset(SAU,SAU$year %in% Start_year:End_year)
SAU <- subset(SAU, !(SAU$functional_group %in% c("Shrimps","Other demersal invertebrates","Cephalopods","Lobsters, crabs","Krill")))

# get total catch
Catch_SAU <- aggregate(tonnes~year,data=SAU,FUN=sum)
colnames(Catch_SAU) <- c("Year","Tonnes")

# get landings
Land_SAU <- subset(SAU,SAU$catch_type == "Landings")
Land_SAU <- aggregate(tonnes~year,data=Land_SAU,FUN=sum)
colnames(Land_SAU) <- c("Year","Tonnes")

# -----------------------------------------------------
# load Watson
load("../Data/Watson/Catch_LME_peldem.RData")
WAT <- subset(comb,comb$LME == LME_name & comb$Year %in% Start_year:End_year)
Catch_WAT <- aggregate(Totcatch~Year,data=WAT,FUN=sum)
colnames(Catch_WAT) <- c("Year","Tonnes")
Land_WAT <- aggregate(Reported~Year,data=WAT,FUN=sum)
colnames(Land_WAT) <- c("Year","Tonnes")

# -----------------------------------------------------
# load stock assessment reported catches
load("../Data/NOAA_stocks/AquaMaps_occurence_NOAA_stocks.Rdata")
load("../Data/NOAA_stocks/Assessment_spatially_allocated.Rdata")
STO <- subset(grid,grid$LME %in% c(LME_name))
STO <- subset(totdat,totdat$id %in% STO$id & totdat$Year %in% Start_year:End_year)
#STO$Catch <- ifelse(is.na(STO$Catch),STO$Landings,STO$Catch)
#STO$Landings <- ifelse(is.na(STO$Landings),STO$Catch,STO$Landings)

Catch_STO <- aggregate(Catch~Year,data=STO,FUN=sum)
colnames(Catch_STO) <- c("Year","Tonnes")

par(mar = c(5, 4, 3, 3),mfrow=c(1,2))
plot(Catch_SAU$Tonnes/10^6~Catch_SAU$Year,ylim=c(0,7),type="l",col="red",lwd=2,
     ylab="Catch (million tonnes y-1)",las=1,xlab="Year",main="Catch",lty=2)
lines(Catch_WAT$Tonnes/10^6~Catch_WAT$Year,col="blue",lwd=2,lty=3)
lines(Catch_STO$Tonnes/10^6~Catch_STO$Year,col="black",lwd=2,lty=1)
legend(Start_year,7.3, legend=c("SAU", "Watson", "Ass. stocks"), col=c("red", "blue","black"), lty=c(2,3,1), cex=1,y.intersp = 1,x.intersp = .5,seg.len=1, box.lty=0,bg="transparent")

plot(Land_SAU$Tonnes/10^6~Land_SAU$Year,ylim=c(0,7),type="l",col="red",lwd=2,
     ylab="Landings (million tonnes y-1)",las=1,xlab="Year",main="Landings",lty=2)
lines(Land_WAT$Tonnes/10^6~Land_WAT$Year,col="blue",lwd=2,lty=3)


```

## Biomass of assessed stocks

Time series of assessed fish biomass were estimated by LME using data from scientific trawl surveys and based on information for all assessed stocks for which total biomass or spawning stock biomass estimates are available in the Gulf of Alaska.

The biomass estimates from scientific surveys were corrected for differences in sampling area and trawl gear catchability (Maureaud et al. 2024; van Denderen et al. 2023). We estimated both total biomass and biomass per unit area using the GLMMTMB package. We only included species for which a stock assessment is provided.

```{r Stock bio.}
#| warning: false
#| message: false
#| error: false

# -----------------------------------------------------
# load biomass from stock assessments
load("../Data/NOAA_stocks/AquaMaps_occurence_NOAA_stocks.Rdata")
load("../Data/NOAA_stocks/Assessment_spatially_allocated.Rdata")

# now many stocks in US are reporting SSB/mature biomass; 
# this is problematic for the comparison (e.g. walleye pollock)
# using Ram v4.44 we estimated per stock a conversion between TB and SSB
# we did not use Ram directly as the time series end before 2010
conv <- read.csv("../Data/NOAA_stocks/ssb_tb_conversion.csv")
totdat <- cbind(totdat,conv[match(totdat$Stock,conv$stocklong), 
                            c("conversion_final")]) 
colnames(totdat)[ncol(totdat)] <- "conversion"
totdat$Tbio <- totdat$Tbio/totdat$conversion

# now combine
MEgrid <- subset(grid,grid$LME %in% c(LME_name))
STO <- subset(totdat,totdat$id %in% MEgrid$id & 
                totdat$Year %in% Start_year:End_year)

# add the area to be able to estimate biomass in mass per area
STO <- cbind(STO,grid[match(STO$id,grid$id), c("area_sqkm","ocean_sqkm","depth")])

# estimate total biomass and get total area
STO <- subset(STO,STO$depth >= -500)
Bio_STO <- aggregate(Tbio~Year,data=STO,FUN=sum)
Bio_STO$area <- sum(MEgrid$ocean_sqkm[MEgrid$depth >= -500])

# -----------------------------------------------------
# load scientific survey biomass
load("../Data/Trawl_survey/Surveys_hauls_species_LMEs.RData")
Reg <- subset(trawl,trawl$Year %in% c(Start_year:End_year))

source("../Processing/Get_grids_per_LME.R")
grid <- get_grid_LME(LME_name = LME_name , grid_size_km = 10)

# match all hauls with the MEgrid
hauls <- Reg[!duplicated(Reg$Haul_id), ]
hauls <- st_as_sf(hauls, coords = c("Longitude", "Latitude"), crs = 4326)
ha_grid <- st_intersects(hauls,grid)
ha_grid <- as.data.frame(ha_grid)

# subset Reg to only get GoA
Reg <- subset(Reg,Reg$Haul_id %in% c(hauls$Haul_id[ha_grid[,1]]))

# now get for each haul and species, if there is a match with a stock
Reg$Stock <- 0 # no match = 0, match = 1

# get fishnames
fishnames <- read.csv("../Data/NOAA_stocks/common_to_scientific_name_cleaned.csv")
name_clean <- gsub("\\.", " ", sub("\\.{3}.*", "", names(st_drop_geometry(MEgrid))))
name_clean <- data.frame(common=name_clean)
name_clean <- cbind(name_clean,fishnames[match(name_clean$common,fishnames$ComName), c("Species")])
colnames(name_clean)[ncol(name_clean)] <- "Latin"

# and match the species per haul to define if they are stocks in that grid cell
Reg$reg_keys <- paste(Reg$Haul_id, Reg$Name)

Reg_sf <- st_as_sf(Reg, coords = c("Longitude", "Latitude"), crs = 4326)
intersections <- as.data.frame(st_intersects(Reg_sf, MEgrid))
Reg_sf$inout  <-  NA
Reg_sf$inout[intersections$row.id] <- MEgrid$id[intersections$col.id]
Reg_sf <- subset(Reg_sf,!(is.na(Reg_sf$inout)))
cells  <- unique(Reg_sf$inout)

for (j in 1:length(cells)) {
  dt <- subset(Reg_sf,Reg_sf$inout == cells[j])
  me_sub <- subset(MEgrid,MEgrid$id == cells[j])
  me_sub <- st_drop_geometry(me_sub)
  
  # For each name in dt, check if any column with that name in MEgrid has a 1
  dt$Stock_flag <- sapply(dt$Name, function(nm) {
    # Get all columns in MEgrid with species name in column name
    colmatch <- grep(nm, name_clean$Latin, fixed = TRUE)
    
    if (length(colmatch) == 0) return(0)
    
    # Check if any intersected cell has a 1 in those columns
    any(me_sub[, colmatch] == 1, na.rm = TRUE)
  })
  
  # Only keep rows where there was a 1 match
  dt <- dt[dt$Stock_flag == 1, ]
  
  if (nrow(dt) > 0) {
    dt_keys  <- paste(dt$Haul_id, dt$Name)
    Reg$Stock <- ifelse(Reg$reg_keys %in% dt_keys, 1,Reg$Stock)
  }
}

Reg$F_type <- ifelse(Reg$F_type =="pel" & Reg$Stock == 1,"pel_stock", Reg$F_type)
Reg$F_type <- ifelse(Reg$F_type =="dem" & Reg$Stock == 1,"dem_stock", Reg$F_type)

# Group and summarize
Reg <- Reg %>% 
  group_by(Haul_id, Survey_Region, Gear, Year, Month, Longitude, 
           Latitude, Bottom_depth, F_type, uniq) %>%
  summarize(kg_km2_corrected = sum(kg_km2_corrected, na.rm = TRUE), 
            .groups = 'drop') %>%
  dplyr::select(Haul_id, Survey_Region, Gear, Year, Month, Longitude, 
           Latitude, Bottom_depth, F_type, uniq, kg_km2_corrected) %>%
  as.data.frame()

# Get estimate by haul
Reg <- Reg %>%
  pivot_wider(names_from = F_type, 
              values_from = kg_km2_corrected, values_fill = 0) %>% 
    mutate(Total_kg_km2 = rowSums(select(., c("dem","pel", #"pel_stock",
                                              "dem_stock")), na.rm = TRUE)) %>%
  as.data.frame()

# and plot surveyed locations by year
ggplot(Reg, aes(x = Longitude, y = Latitude)) +  geom_point(size=0.5) + 
  facet_wrap(~ Year)

# Prepare the spatial grid
grid$col <- ifelse(grid$Depth >= -500,"blue","red")
grid <- subset(grid,grid$lon < -1 & grid$lon > -179.8)
grid <- subset(grid,!(grid$lon < -178 & grid$lat < 65))
plot(grid$geometry, border = NA, col = grid$col, main = "10km Grid")
legend("bottomright",c("in","out"),pch=15,col=c("blue","red"),box.col = "white")


grid <- grid |> 
  rename(Longitude = lon, Latitude = lat) %>%  # Rename columns for clarity
  st_drop_geometry()      # Drop geometry for data manipulation

# Replicate the grid for each unique survey year
grid <- replicate_df(grid, "Year", min(Reg$Year):max(Reg$Year)) 

# Standardize the bottom depth for the grid
grid <- grid |> 
  mutate(log_depth = log(abs(Depth)),
         log_depth_std = (log_depth - mean(log(Reg$Bottom_depth))) / 
           sd(log(Reg$Bottom_depth)))

# Convert longitude and latitude to UTM coordinates
grid <- add_utm_columns(grid, c("Longitude", "Latitude"), units = "km")  
grid <- grid |> na.omit()

# Prepare the survey dataset
Reg <- Reg %>%
  mutate(log_depth_std = scale(log(Bottom_depth))[, 1],  # Standardize bottom depth
    Yearf = as.factor(Year),  # Convert Year to factor
    Surveyf = as.factor(Survey_Region)  # Convert Survey to factor
  )

# Convert longitude and latitude in survey data to UTM coordinates
Reg <- add_utm_columns(Reg, c("Longitude", "Latitude"), units = "km")

# now check if a model is already available
if (!file.exists(paste0("../Models_sdmtmb/stocks_",LME_name,".RData"))) {

  # get stocks biomass 
  Reg$stocks_kg_km2 <- Reg$dem_stock #+ Reg$pel_stock
  
  # get the data type to run the model
  data_type <- "stocks_kg_km2"
  
  # Substitute 0s with the minimum value registered
  # see table(Reg$stocks_kg_km2 ==0)
  #Reg[,data_type] <- ifelse(Reg[,data_type] == 0,
  #                            min(Reg[,data_type][Reg[,data_type] > 0]),
  #                            Reg[,data_type])
  
  # Create a spatial mesh for the data
  spatial_mesh <- make_mesh(Reg, c("X", "Y"), cutoff = 20)
    
  # Fit a spatial model using sdmTMB
  model_fit <- sdmTMB(
    stocks_kg_km2 ~ 0 + Yearf +  Surveyf +
      poly(log_depth_std,2,raw = T), 
    data = Reg,                     
    mesh = spatial_mesh,                
    family = tweedie(link = "log"),       # to deal with some zero's
    spatial = "on",                        # Enable spatial random effects
    time = "Year",                         # Add spatiotemporal random effects
    spatiotemporal = "iid",                 # iid independent, rw as a random walk
  )
  
  # Print model summary for diagnostics
  save(model_fit,file= paste0("../Models_sdmtmb/stocks_",LME_name,".RData"))

} else {
  load(paste0("../Models_sdmtmb/stocks_",LME_name,".RData"))
}

print(model_fit)  

# prepare grid for the prediction
grid$Yearf <- as.factor(grid$Year)
grid$Surveyf <- "Eastern Bering Sea"
grid <- subset(grid, abs(grid$Depth) < 500)
grid <- subset(grid,grid$Year %in% c(unique(Reg$Year)))

# Generate predictions using the fitted model and the grid
predictions <- predict(model_fit, newdata = grid, 
                       return_tmb_object = TRUE) #kg/km2

# make sure that output is back-transformed, if used spatially --> 
# predictions$data$est <- exp(predictions$data$est)
  
# Calculate the biomass index with bias correction
if (!file.exists(paste0("../Models_sdmtmb/Index_stocks_",LME_name,".RData"))) {
  bio_index <- get_index(predictions, area = 100, bias_correct = T)
  save(bio_index,file= paste0("../Models_sdmtmb/Index_stocks_",LME_name,".RData"))
} else {
  load(paste0("../Models_sdmtmb/Index_stocks_",LME_name,".RData"))
}
stck_index <- bio_index
stck_index$area <- length(grid$Year[grid$Year == Start_year]) * 100

plot(Bio_STO$Tbio/Bio_STO$area~Bio_STO$Year,ylim=c(0,60),type="l",col="black",
     lwd=2,ylab="tonnes km-2",las=1,xlab="Year",main="Biomass assessed stocks")
lines(stck_index$est/stck_index$area/1000~stck_index$Year,col="#228833",
      pch=16,lwd=2)
polygon(x = c(stck_index$Year,rev(stck_index$Year)), 
        y = c(stck_index$lwr/stck_index$area/1000,
              rev(stck_index$upr/stck_index$area/1000)),
        col = adjustcolor("#228833", 0.2),border=NA)
legend(Start_year,65, legend=c("Stock database", "Trawl survey"),
       col=c("black", "#228833"), lty=c(1,1), cex=1,y.intersp = 1,x.intersp = .5,
       seg.len=1,box.lty=0,bg="transparent")

```

## Total biomass

Time series of total fish biomass were estimated using data from scientific trawl surveys in a similar way as above but now for all species in the survey.

```{r Tot. biomass}
# -----------------------------------------------------
# continue with the above

# check if a model is already available
if (!file.exists(paste0("../Models_sdmtmb/total_",LME_name,".RData"))) {

  # get the data type to run the model
  data_type <- "Total_kg_km2"
  
  # Substitute 0s with the minimum value registered
  # see table(Reg$Total_kg_km2 == 0)
  #Reg[,data_type] <- ifelse(Reg[,data_type] == 0,
  #                            min(Reg[,data_type][Reg[,data_type] > 0]),
  #                            Reg[,data_type])
  
  # Create a spatial mesh for the data
  spatial_mesh <- make_mesh(Reg, c("X", "Y"), cutoff = 20)
    
  # Fit a spatial model using sdmTMB
  model_fit <- sdmTMB(
    Total_kg_km2 ~ 0 + Yearf + Surveyf +
      poly(log_depth_std,2,raw = T), 
    data = Reg,                     
    mesh = spatial_mesh,                
    family = tweedie(link = "log"),        # Lognormal family
    spatial = "on",                        # Enable spatial random effects
    time = "Year",                         # Add spatiotemporal random effects
    spatiotemporal = "iid",                 # iid independent, rw as a random walk
  )
  
  # Print model summary for diagnostics
  save(model_fit,file= paste0("../Models_sdmtmb/total_",LME_name,".RData"))

} else {
  load(paste0("../Models_sdmtmb/total_",LME_name,".RData"))
}

print(model_fit)  

# grid is already prepared for the prediction

# Generate predictions using the fitted model and the grid
predictions <- predict(model_fit, newdata = grid, 
                       return_tmb_object = TRUE) #kg/km2

# make sure that output is back-transformed, if used spatially --> 
# predictions$data$est <- exp(predictions$data$est)
  
# Calculate the biomass index with bias correction
if (!file.exists(paste0("../Models_sdmtmb/Index_total_",LME_name,".RData"))) {
  bio_index <- get_index(predictions, area = 100, bias_correct = T)
  save(bio_index,file= paste0("../Models_sdmtmb/Index_total_",LME_name,".RData"))
} else {
  load(paste0("../Models_sdmtmb/Index_total_",LME_name,".RData"))
}
bio_index$area <- length(grid$Year[grid$Year == Start_year]) * 100

# make plot
plot(bio_index$est/bio_index$area/1000~bio_index$Year,ylim=c(0,160),type="l",col="#228833",lwd=2,
     ylab="tonnes km-2",las=1,xlab="Year",main="Total survey biomass")
polygon(x = c(bio_index$Year,rev(bio_index$Year)), 
        y = c(bio_index$lwr/bio_index$area/1000,
              rev(bio_index$upr/bio_index$area/1000)),
        col = adjustcolor("#228833", 0.2),border=NA)

```

## Exploitation rate

Time series of the fish community exploitation rate were estimated by ME by dividing catches over total biomass. This was done using catch data from SAU and Watson, survey data and stock assessment information.

```{r Expl. rate}
# -----------------------------------------------------
Catch_SAU_sub <- subset(Catch_SAU,Catch_SAU$Year %in% bio_index$Year)
Catch_SAU_sub$ER <- Catch_SAU_sub$Tonnes/(bio_index$est/1000)
plot(Catch_SAU_sub$ER ~ Catch_SAU_sub$Year, lty=2,lwd=2,col="red",type="l",ylab="yr-1",las=1,xlab="Year",main="Exploitation rate",ylim=c(0,1))

Catch_WAT_sub <- subset(Catch_WAT,Catch_WAT$Year %in% bio_index$Year)
Catch_WAT_sub$ER <- Catch_WAT_sub$Tonnes/(bio_index$est/1000)
lines(Catch_WAT_sub$ER ~ Catch_WAT_sub$Year,lty=3,lwd=2,col="blue")

lines(Catch_STO$Tonnes/Bio_STO$Tbio~Catch_STO$Year,lty=1,lwd=2,col="black")
legend(Start_year,1, legend=c("SAU/survey biomass","WAT/survey biomass",
                              "Stock database"),
       col=c("red", "blue", "black"), lty=c(2,3,1),
       cex=1,y.intersp = 1,x.intersp = .5,
       seg.len=1,box.lty=0,bg="transparent")


```

## Percentage of demersal fish

The proportion of demersal fish can serve as a calibration point for models incorporating different functional groups. This proportion can be derived from both catch statistics and scientific survey data. However, bottom trawl surveys likely underestimate the abundance of pelagic fish, a bias that is partly addressed by assuming lower catchability for these species.

In all Gulf of Alaska statistics, demersal fish account for approximately 90% of the total catch.

```{r perc_demersal}
# -----------------------------------------------------
# get it for SAU
peltype <- c("Small pelagics (<30 cm)","Medium pelagics (30 - 89 cm)",
              "Large pelagics (>=90 cm)","Small bathypelagics (<30 cm)",
             "Medium bathypelagics (30 - 89 cm)","Large bathypelagics (>=90 cm)")
pel_SAU <- subset(SAU,SAU$functional_group %in% peltype)
pel_SAU <- aggregate(tonnes~year,data=pel_SAU,FUN=sum)
colnames(pel_SAU) <- c("Year","Tonnes")
dem_SAU <- subset(SAU,!(SAU$functional_group %in% peltype))
dem_SAU <- aggregate(tonnes~year,data=dem_SAU,FUN=sum)
colnames(dem_SAU) <- c("Year","Tonnes")
pcdem_SAU <- dem_SAU$Tonnes/ (pel_SAU$Tonnes + dem_SAU$Tonnes) * 100

# get if for Watson
pel_WAT <- subset(WAT,WAT$Ftype =="pel")
pel_WAT <- aggregate(Totcatch~Year,data=pel_WAT,FUN=sum)
colnames(pel_WAT) <- c("Year","Tonnes")
dem_WAT <- subset(WAT,WAT$Ftype =="dem")
dem_WAT <- aggregate(Totcatch~Year,data=dem_WAT,FUN=sum)
colnames(dem_WAT) <- c("Year","Tonnes")
pcdem_WAT <- dem_WAT$Tonnes/ (pel_WAT$Tonnes + dem_WAT$Tonnes) * 100

# get it for the survey - shortcut
  agg <- aggregate(
    x = list(dem = Reg$dem, pel = Reg$pel),
    by = list(Year = Reg$Year),
    FUN = sum)
agg$pcdem_SUR <- agg$dem/ (agg$dem + agg$pel) * 100

plot(pcdem_SAU~dem_SAU$Year,ylim=c(0,100),type="l",col="red",
     lwd=2,lty=2,ylab="%",las=1,xlab="Year", main="% demersal in total fish")
lines(pcdem_WAT~dem_WAT$Year,lty=3,lwd=2,col="blue")
lines(agg$pcdem_SUR~agg$Year,lty=1,lwd=2,col="black")
legend(Start_year,35, legend=c("SAU (catch)", "Watson (catch)", "Trawl sur. (biomass)"), col=c("red", "blue","black"), lty=c(2,3,1), cex=1,y.intersp = 1,x.intersp = .5,seg.len=1, box.lty=0,bg="transparent")

```

## Data output

A .csv file is created with all data outputs. Abbreviations: SAU = Sea Around Us, WAT = Watson (2017), SUR = scientific bottom trawl survey data, STO = stock assessment database.

```{r csv output}
# -----------------------------------------------------
# make dataframe with all information

# several have gaps - add NAs
# Create a complete sequence of years
all_years <- data.frame(Year = seq(Start_year,End_year))

# Merge to fill in missing years with NA
stck_index <- merge(all_years, stck_index, by = "Year", all.x = TRUE)
bio_index <- merge(all_years, bio_index, by = "Year", all.x = TRUE)
Catch_WAT_sub <- merge(all_years, Catch_WAT_sub, by = "Year", all.x = TRUE)
Catch_SAU_sub <- merge(all_years, Catch_SAU_sub, by = "Year", all.x = TRUE)
agg <- merge(all_years, agg, by = "Year", all.x = TRUE)

dat <- data.frame(Region = LME_name, 
           Year=Catch_SAU$Year,
           Catch_SAU_tonnes = Catch_SAU$Tonnes,
           Catch_WAT_tonnes = Catch_WAT$Tonnes,
           Catch_STO_tonnes = Catch_STO$Tonnes,
           Land_SAU_tonnes = Land_SAU$Tonnes,
           Land_WAT_tonnes = Land_WAT$Tonnes,
           Land_STO_tonnes = NA,
           Stckbio_lwr_STO_MTkm2 = NA,
           Stckbio_STO_MTkm2 = Bio_STO$Tbio/Bio_STO$area,
           Stckbio_upr_STO_MTkm2 = NA,
           Stckbio_lwr_SUR_MTkm2 = stck_index$lwr/stck_index$area/1000,
           Stckbio_SUR_MTkm2 = stck_index$est/stck_index$area/1000,
           Stckbio_upr_SUR_MTkm2 = stck_index$upr/stck_index$area/1000,
           Tbio_lwr_SUR_MTkm2 = bio_index$lwr/bio_index$area/1000,
           Tbio_SUR_MTkm2 = bio_index$est/bio_index$area/1000,
           Tbio_upr_SUR_MTkm2 = bio_index$upr/bio_index$area/1000,
           ER_SAU_SUR = Catch_SAU_sub$ER,
           ER_WAT_SUR = Catch_WAT_sub$ER,
           ER_STO = Catch_STO$Tonnes/Bio_STO$Tbio,
           pcdem_SAU = pcdem_SAU,
           pcdem_WAT = pcdem_WAT,
           pcdem_SUR = agg$pcdem_SUR)

write.csv(dat, paste0("../Outputs-by-Region/",LME_name,".csv"))
           
```
