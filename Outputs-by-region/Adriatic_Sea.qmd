---
title: "Adriatic Sea biomass and catches"
format: html
embed-resources: true
editor: visual
---

## Goal

To collect, compile, and analyze fish community catch data, landings, and biomass estimates from a variety of sources. This includes integrating datasets from fisheries catch statistics and scientific surveys to quantify catch and biomass across different regions. The goal is to develop a comprehensive dataset for both Large Marine Ecosystems (LMEs) or Marine Ecoregions (MEs) that facilitates the validation and calibration of marine ecosystem models with a focus on fish and fisheries.

## Data types

-   Catches and landings by ME from Sea Around us Data (version 50.1)

-   Catches and landings from Watson (2017, [10.1038/sdata.2017.39](10.1038/sdata.2017.39)) aggregated to ME-scale

-   Fisheries independent trawl survey data of fish biomass (European Commission, Joint Research Centre (2025) corrected with a catchability factor and standardized by swept area (<https://github.com/federico-maioli/prepare_medits>)

```{r}
#| echo: true
#| warning: false
#| message: false
#| error: false

# define values
ME_name   <- "Adriatic Sea"
Start_year <- 1994
End_year <- 2015

# load libraries
library(ggplot2)
library(dplyr)
library(sf)
library(tidyverse)
library(tidyr)
library(sdmTMB)
```

## Catches and landings

Time series of marine fish catches and landings were estimated by ME using data from SAU and Watson (2017). The catch estimates encompass landings, discards, and approximations of illegal, unregulated, and unreported (IUU) catches.

```{r Catch & Land. }
# -----------------------------------------------------
# load SAU data
SAU <- read.csv("../Data/SAU/SAU_AdriaticSea.csv")
SAU <- subset(SAU,SAU$year %in% Start_year:End_year)
SAU <- subset(SAU, !(SAU$functional_group %in% c("Shrimps","Other demersal invertebrates","Cephalopods","Lobsters, crabs","Krill")))

# get total catch
Catch_SAU <- aggregate(tonnes~year,data=SAU,FUN=sum)
colnames(Catch_SAU) <- c("Year","Tonnes")

# get landings
Land_SAU <- subset(SAU,SAU$catch_type == "Landings")
Land_SAU <- aggregate(tonnes~year,data=Land_SAU,FUN=sum)
colnames(Land_SAU) <- c("Year","Tonnes")

# -----------------------------------------------------
# load Watson
load("../Data/Watson/AdriaticME.RData")
WAT <- subset(tot,tot$Year %in% Start_year:End_year)
Catch_WAT <- WAT[,c("Year","Total_catch")]
colnames(Catch_WAT) <- c("Year","Tonnes")
Land_WAT <- WAT[,c("Year","Total_reported")]
colnames(Land_WAT) <- c("Year","Tonnes")

par(mar = c(5, 4, 3, 3),mfrow=c(1,2))
plot(Catch_SAU$Tonnes/10^6~Catch_SAU$Year,ylim=c(0,1.5),type="l",col="red",lwd=2,
     ylab="Catch (million tonnes y-1)",las=1,xlab="Year",main="Catch",lty=2)
lines(Catch_WAT$Tonnes/10^6~Catch_WAT$Year,col="blue",lwd=2,lty=3)
legend(Start_year,1.3, legend=c("SAU", "Watson"), col=c("red", "blue"), lty=c(2,3,1), cex=1,y.intersp = 1,x.intersp = .5,seg.len=1, box.lty=0,bg="transparent")

plot(Land_SAU$Tonnes/10^6~Land_SAU$Year,ylim=c(0,1.5),type="l",col="red",lwd=2,
     ylab="Landings (million tonnes y-1)",las=1,xlab="Year",main="Landings",lty=2)
lines(Land_WAT$Tonnes/10^6~Land_WAT$Year,col="blue",lwd=2,lty=3)


```

## Total biomass

Time series of fish biomass were estimated using data from scientific trawl surveys.

The biomass estimates were corrected for differences in sampling area and trawl gear catchability, processing the publicly available data from the MEDITS (EC, 2025). We estimated both total biomass and biomass per unit area using the GLMMTMB package.

The input data for the statistical model included all available hauls conducted in the Adriatic Sea. Biomass predictions were then projected onto a spatial grid covering areas shallower than 500 meters, representing the primary fishing grounds and the most relevant region for comparison with total catches.

```{r Tot. biomass}
#| warning: false
#| message: false
#| error: false

# load survey biomass
Reg <- read.csv("../Data/Trawl_survey_medits/medits_catch_corrected.csv")
Reg <- subset(Reg,Reg$year %in% c(Start_year:End_year))

source("../Processing/Get_grids_per_ME.R")
grid <- get_grid_ME(ME_name = ME_name , grid_size_km = 10)

# match all hauls with the MEgrid
hauls <- Reg[!duplicated(Reg$haul_id), ]
hauls <- st_as_sf(hauls, coords = c("longitude", "latitude"), crs = 4326)
ha_grid <- st_intersects(hauls,grid)
ha_grid <- as.data.frame(ha_grid)

# subset Reg to only get Adriatic
Reg <- subset(Reg,Reg$haul_id %in% c(hauls$haul_id[ha_grid[,1]]))

# Group and summarize
Reg <- Reg %>% 
  group_by(haul_id, country, gsa, vessel, year, month, longitude, 
           latitude, depth, sp_category) %>%
  summarize(kg_km2_corrected = sum(kg_km2_corrected, na.rm = TRUE), 
            .groups = 'drop') %>%
  dplyr::select(haul_id, country, gsa, vessel, year, month, longitude, 
           latitude, depth, sp_category,kg_km2_corrected) %>%
  as.data.frame()

# Get estimate by haul
Reg <- Reg %>%
  pivot_wider(names_from = sp_category, 
              values_from = kg_km2_corrected, 
              values_fill = 0) %>% 
    mutate(Total_kg_km2 = rowSums(across(c("demersal", "pelagic")), na.rm = TRUE)) %>%
  as.data.frame()

# and plot surveyed locations by year
ggplot(Reg, aes(x = longitude, y = latitude)) +  geom_point(size=0.5) + 
  facet_wrap(~ year)

# Prepare the spatial grid
grid <- subset(grid,!(is.na(grid$Depth)))
grid$col <- ifelse(grid$Depth >= -500,"blue","red")
plot(grid$geometry, border = NA, col = grid$col, main = "10km Grid")
unique_hauls <- Reg[!duplicated(Reg$haul_id), ]
points(unique_hauls$longitude, unique_hauls$latitude)
legend("bottomright",c("in","out"),pch=15,col=c("blue","red"),box.col = "white")

grid <- grid |> 
  rename(Longitude = lon, Latitude = lat) %>%  # Rename columns for clarity
  st_drop_geometry()      # Drop geometry for data manipulation

# Replicate the grid for each unique survey year
grid <- replicate_df(grid, "year", min(Reg$year):max(Reg$year)) 

# Standardize the bottom depth for the grid
grid <- grid |> 
  mutate(log_depth = log(abs(Depth)),
         log_depth_std = (log_depth - mean(log(Reg$depth))) / 
           sd(log(Reg$depth)))

# Convert longitude and latitude to UTM coordinates
grid <- add_utm_columns(grid, c("Longitude", "Latitude"), units = "km")  
grid <- grid |> na.omit()

# Prepare the survey dataset
Reg <- Reg %>%
  mutate(month = as.numeric(month), 
    log_depth_std = scale(log(depth))[, 1],  # Standardize bottom depth
    yearf = as.factor(year),  # Convert Year to factor
    monthf = as.factor(month)) # Convert Month to factor

# Convert longitude and latitude in survey data to UTM coordinates
Reg <- add_utm_columns(Reg, c("longitude", "latitude"), units = "km")

# check if a model is already available
if (!file.exists(paste0("../Models_sdmtmb/total_",ME_name,".RData"))) {

  # get the data type to run the model
  data_type <- "Total_kg_km2"
  
  # Substitute 0s with the minimum value registered
  # see table(Reg$Total_kg_km2 == 0)
  Reg[,data_type] <- ifelse(Reg[,data_type] == 0,
                              min(Reg[,data_type][Reg[,data_type] > 0]),
                              Reg[,data_type])
  
  # Create a spatial mesh for the data
  spatial_mesh <- make_mesh(Reg, c("X", "Y"), cutoff = 20)
    
  # Fit a spatial model using sdmTMB
  model_fit <- sdmTMB(
    Total_kg_km2 ~ 0 + yearf + (1|monthf) + 
      poly(log_depth_std,2,raw = T), 
    data = Reg,                     
    mesh = spatial_mesh,                
    family = lognormal(link = "log"),        # Lognormal family
    spatial = "on",                        # Enable spatial random effects
    time = "year",                         # Add spatiotemporal random effects
    spatiotemporal = "iid",                 # iid independent, rw as a random walk
  )
  
  # Print model summary for diagnostics
  save(model_fit,file= paste0("../Models_sdmtmb/total_",ME_name,".RData"))

} else {
  load(paste0("../Models_sdmtmb/total_",ME_name,".RData"))
}

print(model_fit)  

# prepare grid for the prediction
grid$yearf <- as.factor(grid$year)
grid$monthf <- as.factor(7)
grid <- subset(grid, abs(grid$Depth) < 500)

# grid is already prepared for the prediction

if (!file.exists(paste0("../Models_sdmtmb_Pred/Prediction_",ME_name,".RData"))) {

# Generate predictions using the fitted model and the grid
predictions <- predict(model_fit, newdata = grid, 
                       return_tmb_object = TRUE) #kg/km2

# make sure that output is back-transformed, if used spatially --> 
# predictions$data$est <- exp(predictions$data$est)
  
  save(predictions,file= paste0("../Models_sdmtmb_Pred/Prediction_",ME_name,".RData"))

} else {
  load(paste0("../Models_sdmtmb_Pred/Prediction_",ME_name,".RData"))
}

# Calculate the biomass index with bias correction
if (!file.exists(paste0("../Models_sdmtmb/Index_total_",ME_name,".RData"))) {
  bio_index <- get_index(predictions, area = 100, bias_correct = T)
  save(bio_index,file= paste0("../Models_sdmtmb/Index_total_",ME_name,".RData"))
} else {
  load(paste0("../Models_sdmtmb/Index_total_",ME_name,".RData"))
}
bio_index$area <- length(grid$year[grid$year == Start_year]) * 100

# make plot
plot(bio_index$est/bio_index$area/1000~bio_index$year,ylim=c(0,45),type="l",col="#228833",lwd=2,
     ylab="tonnes km-2",las=1,xlab="Year",main="Total survey biomass")
polygon(x = c(bio_index$year,rev(bio_index$year)), 
        y = c(bio_index$lwr/bio_index$area/1000,
              rev(bio_index$upr/bio_index$area/1000)),
        col = adjustcolor("#228833", 0.2),border=NA)

```

## Exploitation rate

Time series of the fish community exploitation rate were estimated by dividing catches over total biomass. This was done by dividing total SAU or Watson catches with the estimated survey biomass for the area (the blue part of the 10 km grid).

The differences in exploitation rates at the beginning of the time series reflect discrepancies between SAU and Watson catch data.

```{r Expl. rate}
# -----------------------------------------------------
plot((Catch_SAU$Tonnes)/(bio_index$est/1000) ~ Catch_SAU$Year, 
     lty=2,lwd=2,col="red",type="l",ylab="yr-1",las=1,xlab="Year",
     main="Exploitation rate",ylim=c(0,1))

lines((Catch_WAT$Tonnes)/(bio_index$est[bio_index$year %in% c(1995:2015)]/1000) ~ Catch_WAT$Year,lty=3,lwd=2,col="blue")

legend(Start_year,1, legend=c("SAU/survey biomass","WAT/survey biomass"),
       col=c("red", "blue"), lty=c(2,3),
       cex=1,y.intersp = 1,x.intersp = .5,
       seg.len=1,box.lty=0,bg="transparent")


```

## Percentage of demersal fish

The proportion of demersal fish can serve as a calibration point for models incorporating different functional groups. This proportion can be derived from both catch statistics and scientific survey data. However, bottom trawl surveys likely underestimate the abundance of pelagic fish, a bias that is partly addressed by assuming lower catchability for these species.

In the catch statistics, demersal fish account for approximately 40% of the total catch, while in the survey data, they represent around 60%. This discrepancy likely reflects the underestimation of pelagic fish in survey data.

```{r perc_demersal}
# -----------------------------------------------------
# get it for SAU
peltype <- c("Small pelagics (<30 cm)","Medium pelagics (30 - 89 cm)",
              "Large pelagics (>=90 cm)","Small bathypelagics (<30 cm)",
             "Medium bathypelagics (30 - 89 cm)","Large bathypelagics (>=90 cm)")
pel_SAU <- subset(SAU,SAU$functional_group %in% peltype)
pel_SAU <- aggregate(tonnes~year,data=pel_SAU,FUN=sum)
colnames(pel_SAU) <- c("Year","Tonnes")
dem_SAU <- subset(SAU,!(SAU$functional_group %in% peltype))
dem_SAU <- aggregate(tonnes~year,data=dem_SAU,FUN=sum)
colnames(dem_SAU) <- c("Year","Tonnes")
pcdem_SAU <- dem_SAU$Tonnes/ (pel_SAU$Tonnes + dem_SAU$Tonnes) * 100

# get if for Watson
pcdem_WAT <- WAT$Catch_dem/ (WAT$Total_catch) * 100

# get it for the survey - shortcut
  agg <- aggregate(
    x = list(dem = Reg$demersal, pel = Reg$pelagic),
    by = list(Year = Reg$year),
    FUN = sum)
pcdem_SUR <- agg$dem/ (agg$dem + agg$pel) * 100

plot(pcdem_SAU~dem_SAU$Year,ylim=c(0,100),type="l",col="red",
     lwd=2,lty=2,ylab="%",las=1,xlab="Year", main="% demersal in total fish")
lines(pcdem_WAT~WAT$Year,lty=3,lwd=2,col="blue")
lines(pcdem_SUR~agg$Year,lty=1,lwd=2,col="black")
legend(Start_year,100, legend=c("SAU (catch)", "Watson (catch)", "Trawl sur. (biomass)"), col=c("red", "blue","black"), lty=c(2,3,1), cex=1,y.intersp = 1,x.intersp = .5,seg.len=1, box.lty=0,bg="transparent")


```

## Data output

A .csv file is created with all data outputs. Abbreviations: SAU = Sea Around Us, WAT = Watson (2017), SUR = scientific bottom trawl survey data, STO = stock assessment database.

```{r csv output}
# -----------------------------------------------------
# make dataframe with all information 

dat <- data.frame(Region=ME_name,
           Year=Catch_SAU$Year,
           Catch_SAU_tonnes = Catch_SAU$Tonnes,
           Catch_WAT_tonnes = Catch_WAT$Tonnes,
           Catch_STO_tonnes = NA,
           Land_SAU_tonnes = Land_SAU$Tonnes,
           Land_WAT_tonnes = Land_WAT$Tonnes,
           Land_STO_tonnes = NA,
           Stckbio_lwr_STO_MTkm2 = NA,
           Stckbio_STO_MTkm2 = NA,
           Stckbio_upr_STO_MTkm2 = NA,
           Stckbio_lwr_SUR_MTkm2 = NA,
           Stckbio_SUR_MTkm2 = NA,
           Stckbio_upr_SUR_MTkm2 = NA,
           Tbio_lwr_SUR_MTkm2 = bio_index$lwr/bio_index$area/1000,
           Tbio_SUR_MTkm2 = bio_index$est/bio_index$area/1000,
           Tbio_upr_SUR_MTkm2 = bio_index$upr/bio_index$area/1000,
           ER_SAU_SUR = Catch_SAU$Tonnes/(bio_index$est/1000),
           ER_WAT_SUR = Catch_WAT$Tonnes/(bio_index$est/1000),
           ER_STO = NA,
           pcdem_SAU = pcdem_SAU,
           pcdem_WAT = pcdem_WAT,
           pcdem_SUR = pcdem_SUR)

write.csv(dat, paste0("../Outputs-by-region/",ME_name,".csv"),row.names = F)
           
```
